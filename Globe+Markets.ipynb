{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Goal\n",
    "\n",
    "With this project I will try to train a neural network to predict whether the stock market closing index S&P 500 will rise or fall on a particular date, based on most recent previous values and other stock markets around the world (where some markets close earlier that day because of time difference).\n",
    "\n",
    "- All Ords (Australlia), closing EST= 01am\n",
    "- Nikkei 225 (Japan), closing EST= 02am\n",
    "- Hang Seng (Hong Kong), closing EST= 04am\n",
    "- DAX (Germany), closing EST= 11:30am\n",
    "- NYSE Composite (US), closing EST= 04pm\n",
    "- Dow Jones Industrial Average (US), closing EST= 04pm\n",
    "- S&P 500 (US), closing EST= 04pm\n",
    "\n",
    "We can use closing price of All Ords, Nikkei 225, Hang Seng and DAX to predict closing price of S&P 500 on the same day, since they close earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import quandl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import functools\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quandl.ApiConfig.api_key = \"7w6LeRcCC_kYpMy4tMpw\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "(Status 404) (Quandl Error QECx02) You have submitted an incorrect Quandl code. Please check your Quandl codes and try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-9580947ab45a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_aus_aord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquandl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"YAHOO/INDEX_AORD\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"2012-05-12\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata_jap_n225\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquandl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"YAHOO/INDEX_N225\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"2012-05-12\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata_hk_hs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquandl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"YAHOO/INDEX_HSI\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"2012-05-12\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata_ger_dax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquandl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"YAHOO/INDEX_GDAXI\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"2012-05-12\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata_us_nyse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquandl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"YAHOO/INDEX_NYA\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"2012-05-12\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/quandl/get.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(dataset, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdataset_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'column_index'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'column_index'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdataset_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'column_index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'code'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_column_not_found\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;31m# Array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/quandl/model/dataset.py\u001b[0m in \u001b[0;36mdata\u001b[0;34m(self, **options)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mupdated_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mupdated_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhandle_not_found_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/quandl/operations/list.py\u001b[0m in \u001b[0;36mall\u001b[0;34m(cls, **options)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstructed_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mresponse_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_dates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/quandl/connection.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(cls, http_verb, url, **options)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mabs_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%s/%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mApiConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_verb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabs_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/quandl/connection.py\u001b[0m in \u001b[0;36mexecute_request\u001b[0;34m(cls, http_verb, url, **options)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_api_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/quandl/connection.py\u001b[0m in \u001b[0;36mhandle_api_error\u001b[0;34m(cls, resp)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_klass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_letter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQuandlError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m: (Status 404) (Quandl Error QECx02) You have submitted an incorrect Quandl code. Please check your Quandl codes and try again."
     ]
    }
   ],
   "source": [
    "data_aus_aord = quandl.get(\"YAHOO/INDEX_AORD\", start_date=\"2012-05-12\")\n",
    "data_jap_n225 = quandl.get(\"YAHOO/INDEX_N225\", start_date=\"2012-05-12\")\n",
    "data_hk_hs = quandl.get(\"YAHOO/INDEX_HSI\", start_date=\"2012-05-12\")\n",
    "data_ger_dax = quandl.get(\"YAHOO/INDEX_GDAXI\", start_date=\"2012-05-12\")\n",
    "data_us_nyse = quandl.get(\"YAHOO/INDEX_NYA\", start_date=\"2012-05-12\")\n",
    "data_us_dji = quandl.get(\"YAHOO/INDEX_DJI\", start_date=\"2012-05-12\")\n",
    "data_us_sap = quandl.get(\"YAHOO/INDEX_GSPC\", start_date=\"2012-05-12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Cut only interesting featrues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_aus_aord' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-744ce7ece04d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclosing_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclosing_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'aord_close'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_aus_aord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Adjusted Close'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mclosing_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n225_close'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_jap_n225\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Adjusted Close'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mclosing_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hs_close'\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mdata_hk_hs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Adjusted Close'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_aus_aord' is not defined"
     ]
    }
   ],
   "source": [
    "closing_data = pd.DataFrame()\n",
    "\n",
    "closing_data['aord_close'] = data_aus_aord['Adjusted Close']\n",
    "closing_data['n225_close'] = data_jap_n225['Adjusted Close']\n",
    "closing_data['hs_close']   = data_hk_hs['Adjusted Close']\n",
    "closing_data['dax_close']  = data_ger_dax['Adjusted Close']\n",
    "closing_data['nyse_close'] = data_us_nyse['Adjusted Close']\n",
    "closing_data['dji_close']  = data_us_dji['Adjusted Close']\n",
    "closing_data['sap_close']  = data_us_sap['Adjusted Close']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a) Remove unsuitable data (Hang Seng Hong Kong slip down around 18th April 2017)\n",
    "closing_data = closing_data.drop([pd.Timestamp('2017-04-18')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# b) Fill gaps in the data.\n",
    "closing_data = closing_data.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe data\n",
    "closing_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe data in graph\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(closing_data)\n",
    "plt.show()\n",
    "\n",
    "# => data needs to be modifiend to fit the same scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# c) Normalize data to fit the same scale\n",
    "closing_data_norm = pd.DataFrame()\n",
    "\n",
    "closing_data_norm['aord_close'] = closing_data['aord_close'] / max(closing_data['aord_close'])\n",
    "closing_data_norm['n225_close'] = closing_data['n225_close'] / max(closing_data['n225_close'])\n",
    "closing_data_norm['hs_close']   = closing_data['hs_close']   / max(closing_data['hs_close'])\n",
    "closing_data_norm['dax_close']  = closing_data['dax_close']  / max(closing_data['dax_close'])\n",
    "closing_data_norm['nyse_close'] = closing_data['nyse_close'] / max(closing_data['nyse_close'])\n",
    "closing_data_norm['dji_close']  = closing_data['dji_close']  / max(closing_data['dji_close'])\n",
    "closing_data_norm['sap_close']  = closing_data['sap_close']  / max(closing_data['sap_close'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe trend in data in graph\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(closing_data_norm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe corelations in data in graph\n",
    "fig = plt.figure()\n",
    "fig.set_figwidth(20)\n",
    "fig.set_figheight(15)\n",
    "\n",
    "_ = pd.plotting.autocorrelation_plot(closing_data_norm['aord_close'], label='aord_close')\n",
    "_ = pd.plotting.autocorrelation_plot(closing_data_norm['n225_close'], label='n225_close')\n",
    "_ = pd.plotting.autocorrelation_plot(closing_data_norm['hs_close'], label='hs_close')\n",
    "_ = pd.plotting.autocorrelation_plot(closing_data_norm['dax_close'], label='dax_close')\n",
    "_ = pd.plotting.autocorrelation_plot(closing_data_norm['nyse_close'], label='nyse_close')\n",
    "_ = pd.plotting.autocorrelation_plot(closing_data_norm['dji_close'], label='dji_close')\n",
    "_ = pd.plotting.autocorrelation_plot(closing_data_norm['sap_close'], label='sap_close')\n",
    "\n",
    "_ = plt.legend(loc='best')\n",
    "\n",
    "# => strong corelations in smaller lag (up to ~400 lag), this meand resent past data corelates with upcomming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe corelations in data in matrix graph\n",
    "_ = pd.plotting.scatter_matrix(closing_data_norm, figsize=(10, 10), diagonal='kde')\n",
    "\n",
    "# => we can see there is significant corellation between market indexes on the same day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make time series data stationary in the mean, thus having no trend in the data\n",
    "# aplying log(Vt/Vt-1) on all data\n",
    "closing_data_norm_log = pd.DataFrame()\n",
    "\n",
    "closing_data_norm_log['aord_close'] = np.log(closing_data['aord_close'] / closing_data['aord_close'].shift())\n",
    "closing_data_norm_log['n225_close'] = np.log(closing_data['n225_close'] / closing_data['n225_close'].shift())\n",
    "closing_data_norm_log['hs_close'] = np.log(closing_data['hs_close'] / closing_data['hs_close'].shift())\n",
    "closing_data_norm_log['dax_close'] = np.log(closing_data['dax_close'] / closing_data['dax_close'].shift())\n",
    "closing_data_norm_log['nyse_close'] = np.log(closing_data['nyse_close'] / closing_data['nyse_close'].shift())\n",
    "closing_data_norm_log['dji_close'] = np.log(closing_data['dji_close'] / closing_data['dji_close'].shift())\n",
    "closing_data_norm_log['sap_close'] = np.log(closing_data['sap_close'] / closing_data['sap_close'].shift())\n",
    "\n",
    "# remove first row (contains NaN because of the t-1 shift)\n",
    "closing_data_norm_log = closing_data_norm_log.iloc[1:]\n",
    "closing_data_norm_log.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Step 2: Create training/testing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. create labels (one-hot encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closing_data_norm_log['sap_rising'] = 0\n",
    "closing_data_norm_log['sap_falling'] = 0\n",
    "\n",
    "closing_data_norm_log.loc[closing_data_norm_log['sap_close'] >= 0, 'sap_rising'] = 1\n",
    "closing_data_norm_log.loc[closing_data_norm_log['sap_close'] < 0, 'sap_falling'] = 1\n",
    "\n",
    "closing_data_norm_log.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. pick features and design dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There will be 2 types of features to predict the market S&P 500 on timestamp T:\n",
    "- features from markets that close before the market we are predicting (that means All Ords,Nikkei 225,Hang Seng and DAX), for this case we can use the closing value on day T since the market closed earlier, and T-1, T-2 (closing value from previous days)\n",
    "- features from markets that close at the same time or after the predicted (for S&P 500 that will be the rest of the US market: Dow Jones Industrial Average, NYSE Composite), for this case we use T-1,T-2,T-3 values (values from 3 preceding dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# including labels one-hot encoded (sap_rising, sap_falling)\n",
    "feature_columns = ['sap_rising', 'sap_falling',\n",
    "                  'aord_close_t0', 'aord_close_t1', 'aord_close_t2',\n",
    "                  'n225_close_t0', 'n225_close_t1', 'n225_close_t2',\n",
    "                  'hs_close_t0', 'hs_close_t1', 'hs_close_t2',\n",
    "                  'dax_close_t0', 'dax_close_t1', 'dax_close_t2',\n",
    "                  'nyse_close_t1', 'nyse_close_t2', 'nyse_close_t3',\n",
    "                  'dji_close_t1', 'dji_close_t2', 'dji_close_t3',\n",
    "                  'sap_close_t1', 'sap_close_t2', 'sap_close_t3']\n",
    "dataset = pd.DataFrame(columns=feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compose dataset from features\n",
    "for i in range(3, len(closing_data_norm_log)):\n",
    "    dataset = dataset.append({\n",
    "        'sap_rising':    closing_data_norm_log.iloc[i]['sap_rising'],\n",
    "        'sap_falling':   closing_data_norm_log.iloc[i]['sap_falling'],\n",
    "        'aord_close_t0': closing_data_norm_log.iloc[i]['aord_close'],\n",
    "        'aord_close_t1': closing_data_norm_log.iloc[i-1]['aord_close'],\n",
    "        'aord_close_t2': closing_data_norm_log.iloc[i-2]['aord_close'],\n",
    "        'n225_close_t0': closing_data_norm_log.iloc[i]['n225_close'],\n",
    "        'n225_close_t1': closing_data_norm_log.iloc[i-1]['n225_close'],\n",
    "        'n225_close_t2': closing_data_norm_log.iloc[i-2]['n225_close'],\n",
    "        'hs_close_t0':   closing_data_norm_log.iloc[i]['hs_close'],\n",
    "        'hs_close_t1':   closing_data_norm_log.iloc[i-1]['hs_close'],\n",
    "        'hs_close_t2':   closing_data_norm_log.iloc[i-2]['hs_close'],\n",
    "        'dax_close_t0':  closing_data_norm_log.iloc[i]['dax_close'],\n",
    "        'dax_close_t1':  closing_data_norm_log.iloc[i-1]['dax_close'],\n",
    "        'dax_close_t2':  closing_data_norm_log.iloc[i-2]['dax_close'],\n",
    "        'nyse_close_t1': closing_data_norm_log.iloc[i-1]['nyse_close'],\n",
    "        'nyse_close_t2': closing_data_norm_log.iloc[i-2]['nyse_close'],\n",
    "        'nyse_close_t3': closing_data_norm_log.iloc[i-3]['nyse_close'],\n",
    "        'dji_close_t1':  closing_data_norm_log.iloc[i-1]['dji_close'],\n",
    "        'dji_close_t2':  closing_data_norm_log.iloc[i-2]['dji_close'],\n",
    "        'dji_close_t3':  closing_data_norm_log.iloc[i-3]['dji_close'],\n",
    "        'sap_close_t1':  closing_data_norm_log.iloc[i-1]['sap_close'],\n",
    "        'sap_close_t2':  closing_data_norm_log.iloc[i-2]['sap_close'],\n",
    "        'sap_close_t3':  closing_data_norm_log.iloc[i-3]['sap_close']},\n",
    "        ignore_index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataProvider():\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.ctr = 0\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # split training/testing according to ratio (default 0.8)\n",
    "        train_set_size = int(len(dataset) * 0.8)\n",
    "        test_set_size = len(dataset) - train_set_size\n",
    "\n",
    "        self.training_dataset = dataset[:train_set_size]\n",
    "        self.testing_dataset  = dataset[train_set_size:]\n",
    "\n",
    "        # split labels\n",
    "        self.training_labels = self.training_dataset[self.training_dataset.columns[:2]]\n",
    "        self.training_dataset = self.training_dataset[self.training_dataset.columns[2:]]\n",
    "        self.testing_labels = self.testing_dataset[self.testing_dataset.columns[:2]]\n",
    "        self.testing_dataset = self.testing_dataset[self.testing_dataset.columns[2:]]\n",
    "        \n",
    "    def next_batch_train(self):\n",
    "        begin_position = self.ctr * self.batch_size\n",
    "        \n",
    "        if begin_position + self.batch_size >= len(self.training_dataset):\n",
    "            data = self.training_dataset[begin_position:]\n",
    "            label = self.training_labels[begin_position:]\n",
    "            self.ctr = 0\n",
    "        else:\n",
    "            data = self.training_dataset[begin_position:begin_position + self.batch_size]\n",
    "            label = self.training_labels[begin_position:begin_position + self.batch_size]\n",
    "            self.ctr += 1\n",
    "        \n",
    "        return data.values, label.values\n",
    "    \n",
    "    def get_test_data(self):\n",
    "        return self.testing_dataset.values, self.testing_labels.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# config\n",
    "no_of_iterations = 50000\n",
    "batch_size = 200\n",
    "\n",
    "## model config\n",
    "hidden_layer1_neurons = 60\n",
    "hidden_layer2_neurons = 30\n",
    "hidden_layer3_neurons = 20\n",
    "\n",
    "# DropOut\n",
    "pkeep_train = 0.75\n",
    "\n",
    "# number of features\n",
    "input_dim = len(dataset.columns) - 2\n",
    "\n",
    "# number of output classes\n",
    "output_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_provider = DataProvider(dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# custom decorator for Model\n",
    "#  - to make functions execute only the first time (every time the functions are called, the graph would be extended by new code)\n",
    "#  - name the variable scope for TF visualization\n",
    "def define_scope(function, scope=None):\n",
    "    attribute = '_cache_' + function.__name__\n",
    "\n",
    "    name = scope or function.__name__\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            with tf.variable_scope(name):\n",
    "                setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model description:\n",
    "Model consists of 3 hidden layers + 1 softmax output layer.\n",
    "Incorporates shootout while training to make inidividual neurons more independent on other layers and perform better.\n",
    "Incorporates learning rate decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, data, label, learning_rate):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.learning_rate = learning_rate\n",
    "        self.prediction\n",
    "        self.optimize\n",
    "        \n",
    "    @define_scope\n",
    "    def prediction(self):        \n",
    "        # weights + biases\n",
    "        w1 = tf.Variable(tf.truncated_normal([input_dim, hidden_layer1_neurons], stddev=0.0001))\n",
    "        b1 = tf.Variable(tf.ones([hidden_layer1_neurons]))\n",
    "\n",
    "        w2 = tf.Variable(tf.truncated_normal([hidden_layer1_neurons, hidden_layer2_neurons], stddev=0.0001))\n",
    "        b2 = tf.Variable(tf.ones([hidden_layer2_neurons]))\n",
    "\n",
    "        w3 = tf.Variable(tf.truncated_normal([hidden_layer2_neurons, hidden_layer3_neurons], stddev=0.0001))\n",
    "        b3 = tf.Variable(tf.ones([hidden_layer3_neurons]))\n",
    "        \n",
    "        w4 = tf.Variable(tf.truncated_normal([hidden_layer3_neurons, output_dim], stddev=0.0001))\n",
    "        b4 = tf.Variable(tf.ones([output_dim]))\n",
    "        \n",
    "        # hidden layers\n",
    "        Y1 = tf.nn.relu(tf.matmul(self.data, w1) + b1)\n",
    "        Y1d = tf.nn.dropout(Y1, pkeep)\n",
    "        Y2 = tf.nn.relu(tf.matmul(Y1, w2) + b2)\n",
    "        Y2d = tf.nn.dropout(Y2, pkeep)\n",
    "        Y3 = tf.nn.relu(tf.matmul(Y2, w3) + b3)\n",
    "        Y3d = tf.nn.dropout(Y3, pkeep)\n",
    "        \n",
    "        # softmax layer\n",
    "        return tf.nn.softmax(tf.matmul(Y3d, w4) + b4)\n",
    "    \n",
    "    @define_scope\n",
    "    def optimize(self):\n",
    "        # compute cost function and minimize\n",
    "        cross_entropy = -tf.reduce_sum(self.label * tf.log(self.prediction))\n",
    "        return tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy), cross_entropy\n",
    "    \n",
    "    @define_scope\n",
    "    def error(self):\n",
    "        mistakes = tf.equal(tf.argmax(self.label, 1), tf.argmax(self.prediction, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(mistakes, tf.float32))\n",
    "        loss = -tf.reduce_sum(self.label * tf.log(self.prediction))\n",
    "        return accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data feed\n",
    "X = tf.placeholder(tf.float32, [None, input_dim])\n",
    "_Y = tf.placeholder(tf.float32, [None, output_dim])\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "\n",
    "# DropOut: feed in 1 when testing, 0.75 when training\n",
    "pkeep = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(data=X, label=_Y, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "accuracy = []\n",
    "_loss = []\n",
    "for i in range(no_of_iterations):\n",
    "    # execute training step\n",
    "    # optimizer learning rate decay\n",
    "    lrmax = 0.001\n",
    "    lrmin = 0.00001\n",
    "    lr = lrmin + (lrmax - lrmin) * math.exp(-i / 2000)\n",
    "    \n",
    "    data_batch, label_batch = data_provider.next_batch_train()\n",
    "    sess.run(model.optimize, feed_dict={X: data_batch, _Y: label_batch, learning_rate:lr, pkeep: pkeep_train})\n",
    "    \n",
    "    if i % 500 == 0:\n",
    "        # compute accuracy\n",
    "        data_batch, label_batch = data_provider.get_test_data()\n",
    "        acc, loss = sess.run(model.error, feed_dict={X: data_batch, _Y: label_batch, pkeep: 1})\n",
    "        accuracy.append(acc)\n",
    "        _loss.append(loss)\n",
    "        print('---epoch {}---\\naccuracy: {}, loss: {}'.format(i // 500, acc, loss))\n",
    "        \n",
    "\n",
    "# accuracy on test data\n",
    "data_batch, label_batch = data_provider.get_test_data()\n",
    "acc, loss = sess.run(model.error, feed_dict={X: data_batch, _Y: label_batch, pkeep: 1})\n",
    "accuracy.append(acc)\n",
    "_loss.append(loss)\n",
    "print('Training finished\\naccuracy: {}, loss: {}'.format(acc, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,6))\n",
    "\n",
    "# accuracy\n",
    "plt.subplot(211)\n",
    "plt.plot(accuracy)\n",
    "\n",
    "# loss\n",
    "plt.subplot(212)\n",
    "plt.plot(np.log(_loss))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With current settings the deep neural network performs with accuracy close to 70%. This number could be potentially further increased by introducing more data (more features or larger training dataset) or by tweaking Model parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
