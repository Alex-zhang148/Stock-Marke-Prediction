{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Goal\n",
    "\n",
    "With this project I will try to train a neural network to predict whether the stock market closing index S&P 500 will rise or fall on a particular date, based on most recent previous values and other stock markets around the world (where some markets close earlier that day because of time difference).\n",
    "\n",
    "- All Ords (Australlia), closing EST= 01am\n",
    "- Nikkei 225 (Japan), closing EST= 02am\n",
    "- Hang Seng (Hong Kong), closing EST= 04am\n",
    "- DAX (Germany), closing EST= 11:30am\n",
    "- NYSE Composite (US), closing EST= 04pm\n",
    "- Dow Jones Industrial Average (US), closing EST= 04pm\n",
    "- S&P 500 (US), closing EST= 04pm\n",
    "\n",
    "We can use closing price of All Ords, Nikkei 225, Hang Seng and DAX to predict closing price of S&P 500 on the same day, since they close earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import quandl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import functools\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quandl.ApiConfig.api_key = \"7w6LeRcCC_kYpMy4tMpw\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aus_aord = quandl.get(\"YAHOO/INDEX_AORD\", start_date=\"2012-05-12\")\n",
    "data_jap_n225 = quandl.get(\"YAHOO/INDEX_N225\", start_date=\"2012-05-12\")\n",
    "data_hk_hs = quandl.get(\"YAHOO/INDEX_HSI\", start_date=\"2012-05-12\")\n",
    "data_ger_dax = quandl.get(\"YAHOO/INDEX_GDAXI\", start_date=\"2012-05-12\")\n",
    "data_us_nyse = quandl.get(\"YAHOO/INDEX_NYA\", start_date=\"2012-05-12\")\n",
    "data_us_dji = quandl.get(\"YAHOO/INDEX_DJI\", start_date=\"2012-05-12\")\n",
    "data_us_sap = quandl.get(\"YAHOO/INDEX_GSPC\", start_date=\"2012-05-12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Cut only interesting featrues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closing_data = pd.DataFrame()\n",
    "\n",
    "closing_data['aord_close'] = data_aus_aord['Adjusted Close']\n",
    "closing_data['n225_close'] = data_jap_n225['Adjusted Close']\n",
    "closing_data['hs_close']   = data_hk_hs['Adjusted Close']\n",
    "closing_data['dax_close']  = data_ger_dax['Adjusted Close']\n",
    "closing_data['nyse_close'] = data_us_nyse['Adjusted Close']\n",
    "closing_data['dji_close']  = data_us_dji['Adjusted Close']\n",
    "closing_data['sap_close']  = data_us_sap['Adjusted Close']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a) Remove unsuitable data (Hang Seng Hong Kong slip down around 18th April 2017)\n",
    "closing_data = closing_data.drop([pd.Timestamp('2017-04-18')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# b) Fill gaps in the data.\n",
    "closing_data = closing_data.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe data\n",
    "closing_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe data in graph\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(closing_data)\n",
    "plt.show()\n",
    "\n",
    "# => data needs to be modifiend to fit the same scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# c) Normalize data to fit the same scale\n",
    "closing_data_norm = pd.DataFrame()\n",
    "\n",
    "closing_data_norm['aord_close'] = closing_data['aord_close'] / max(closing_data['aord_close'])\n",
    "closing_data_norm['n225_close'] = closing_data['n225_close'] / max(closing_data['n225_close'])\n",
    "closing_data_norm['hs_close']   = closing_data['hs_close']   / max(closing_data['hs_close'])\n",
    "closing_data_norm['dax_close']  = closing_data['dax_close']  / max(closing_data['dax_close'])\n",
    "closing_data_norm['nyse_close'] = closing_data['nyse_close'] / max(closing_data['nyse_close'])\n",
    "closing_data_norm['dji_close']  = closing_data['dji_close']  / max(closing_data['dji_close'])\n",
    "closing_data_norm['sap_close']  = closing_data['sap_close']  / max(closing_data['sap_close'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe trend in data in graph\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(closing_data_norm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe corelations in data in graph\n",
    "fig = plt.figure()\n",
    "fig.set_figwidth(20)\n",
    "fig.set_figheight(15)\n",
    "\n",
    "_ = pd.plotting.autocorrelation_plot(closing_data_norm['aord_close'], label='aord_close')\n",
    "_ = pd.plotting.autocorrelation_plot(closing_data_norm['n225_close'], label='n225_close')\n",
    "_ = pd.plotting.autocorrelation_plot(closing_data_norm['hs_close'], label='hs_close')\n",
    "_ = pd.plotting.autocorrelation_plot(closing_data_norm['dax_close'], label='dax_close')\n",
    "_ = pd.plotting.autocorrelation_plot(closing_data_norm['nyse_close'], label='nyse_close')\n",
    "_ = pd.plotting.autocorrelation_plot(closing_data_norm['dji_close'], label='dji_close')\n",
    "_ = pd.plotting.autocorrelation_plot(closing_data_norm['sap_close'], label='sap_close')\n",
    "\n",
    "_ = plt.legend(loc='best')\n",
    "\n",
    "# => strong corelations in smaller lag (up to ~400 lag), this meand resent past data corelates with upcomming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe corelations in data in matrix graph\n",
    "_ = pd.plotting.scatter_matrix(closing_data_norm, figsize=(10, 10), diagonal='kde')\n",
    "\n",
    "# => we can see there is significant corellation between market indexes on the same day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make time series data stationary in the mean, thus having no trend in the data\n",
    "# aplying log(Vt/Vt-1) on all data\n",
    "closing_data_norm_log = pd.DataFrame()\n",
    "\n",
    "closing_data_norm_log['aord_close'] = np.log(closing_data['aord_close'] / closing_data['aord_close'].shift())\n",
    "closing_data_norm_log['n225_close'] = np.log(closing_data['n225_close'] / closing_data['n225_close'].shift())\n",
    "closing_data_norm_log['hs_close'] = np.log(closing_data['hs_close'] / closing_data['hs_close'].shift())\n",
    "closing_data_norm_log['dax_close'] = np.log(closing_data['dax_close'] / closing_data['dax_close'].shift())\n",
    "closing_data_norm_log['nyse_close'] = np.log(closing_data['nyse_close'] / closing_data['nyse_close'].shift())\n",
    "closing_data_norm_log['dji_close'] = np.log(closing_data['dji_close'] / closing_data['dji_close'].shift())\n",
    "closing_data_norm_log['sap_close'] = np.log(closing_data['sap_close'] / closing_data['sap_close'].shift())\n",
    "\n",
    "# remove first row (contains NaN because of the t-1 shift)\n",
    "closing_data_norm_log = closing_data_norm_log.iloc[1:]\n",
    "closing_data_norm_log.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Step 2: Create training/testing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. create labels (one-hot encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closing_data_norm_log['sap_rising'] = 0\n",
    "closing_data_norm_log['sap_falling'] = 0\n",
    "\n",
    "closing_data_norm_log.loc[closing_data_norm_log['sap_close'] >= 0, 'sap_rising'] = 1\n",
    "closing_data_norm_log.loc[closing_data_norm_log['sap_close'] < 0, 'sap_falling'] = 1\n",
    "\n",
    "closing_data_norm_log.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. pick features and design dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There will be 2 types of features to predict the market S&P 500 on timestamp T:\n",
    "- features from markets that close before the market we are predicting (that means All Ords,Nikkei 225,Hang Seng and DAX), for this case we can use the closing value on day T since the market closed earlier, and T-1, T-2 (closing value from previous days)\n",
    "- features from markets that close at the same time or after the predicted (for S&P 500 that will be the rest of the US market: Dow Jones Industrial Average, NYSE Composite), for this case we use T-1,T-2,T-3 values (values from 3 preceding dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# including labels one-hot encoded (sap_rising, sap_falling)\n",
    "feature_columns = ['sap_rising', 'sap_falling',\n",
    "                  'aord_close_t0', 'aord_close_t1', 'aord_close_t2',\n",
    "                  'n225_close_t0', 'n225_close_t1', 'n225_close_t2',\n",
    "                  'hs_close_t0', 'hs_close_t1', 'hs_close_t2',\n",
    "                  'dax_close_t0', 'dax_close_t1', 'dax_close_t2',\n",
    "                  'nyse_close_t1', 'nyse_close_t2', 'nyse_close_t3',\n",
    "                  'dji_close_t1', 'dji_close_t2', 'dji_close_t3',\n",
    "                  'sap_close_t1', 'sap_close_t2', 'sap_close_t3']\n",
    "dataset = pd.DataFrame(columns=feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compose dataset from features\n",
    "for i in range(3, len(closing_data_norm_log)):\n",
    "    dataset = dataset.append({\n",
    "        'sap_rising':    closing_data_norm_log.iloc[i]['sap_rising'],\n",
    "        'sap_falling':   closing_data_norm_log.iloc[i]['sap_falling'],\n",
    "        'aord_close_t0': closing_data_norm_log.iloc[i]['aord_close'],\n",
    "        'aord_close_t1': closing_data_norm_log.iloc[i-1]['aord_close'],\n",
    "        'aord_close_t2': closing_data_norm_log.iloc[i-2]['aord_close'],\n",
    "        'n225_close_t0': closing_data_norm_log.iloc[i]['n225_close'],\n",
    "        'n225_close_t1': closing_data_norm_log.iloc[i-1]['n225_close'],\n",
    "        'n225_close_t2': closing_data_norm_log.iloc[i-2]['n225_close'],\n",
    "        'hs_close_t0':   closing_data_norm_log.iloc[i]['hs_close'],\n",
    "        'hs_close_t1':   closing_data_norm_log.iloc[i-1]['hs_close'],\n",
    "        'hs_close_t2':   closing_data_norm_log.iloc[i-2]['hs_close'],\n",
    "        'dax_close_t0':  closing_data_norm_log.iloc[i]['dax_close'],\n",
    "        'dax_close_t1':  closing_data_norm_log.iloc[i-1]['dax_close'],\n",
    "        'dax_close_t2':  closing_data_norm_log.iloc[i-2]['dax_close'],\n",
    "        'nyse_close_t1': closing_data_norm_log.iloc[i-1]['nyse_close'],\n",
    "        'nyse_close_t2': closing_data_norm_log.iloc[i-2]['nyse_close'],\n",
    "        'nyse_close_t3': closing_data_norm_log.iloc[i-3]['nyse_close'],\n",
    "        'dji_close_t1':  closing_data_norm_log.iloc[i-1]['dji_close'],\n",
    "        'dji_close_t2':  closing_data_norm_log.iloc[i-2]['dji_close'],\n",
    "        'dji_close_t3':  closing_data_norm_log.iloc[i-3]['dji_close'],\n",
    "        'sap_close_t1':  closing_data_norm_log.iloc[i-1]['sap_close'],\n",
    "        'sap_close_t2':  closing_data_norm_log.iloc[i-2]['sap_close'],\n",
    "        'sap_close_t3':  closing_data_norm_log.iloc[i-3]['sap_close']},\n",
    "        ignore_index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataProvider():\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.ctr = 0\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # split training/testing according to ratio (default 0.8)\n",
    "        train_set_size = int(len(dataset) * 0.8)\n",
    "        test_set_size = len(dataset) - train_set_size\n",
    "\n",
    "        self.training_dataset = dataset[:train_set_size]\n",
    "        self.testing_dataset  = dataset[train_set_size:]\n",
    "\n",
    "        # split labels\n",
    "        self.training_labels = self.training_dataset[self.training_dataset.columns[:2]]\n",
    "        self.training_dataset = self.training_dataset[self.training_dataset.columns[2:]]\n",
    "        self.testing_labels = self.testing_dataset[self.testing_dataset.columns[:2]]\n",
    "        self.testing_dataset = self.testing_dataset[self.testing_dataset.columns[2:]]\n",
    "        \n",
    "    def next_batch_train(self):\n",
    "        begin_position = self.ctr * self.batch_size\n",
    "        \n",
    "        if begin_position + self.batch_size >= len(self.training_dataset):\n",
    "            data = self.training_dataset[begin_position:]\n",
    "            label = self.training_labels[begin_position:]\n",
    "            self.ctr = 0\n",
    "        else:\n",
    "            data = self.training_dataset[begin_position:begin_position + self.batch_size]\n",
    "            label = self.training_labels[begin_position:begin_position + self.batch_size]\n",
    "            self.ctr += 1\n",
    "        \n",
    "        return data.values, label.values\n",
    "    \n",
    "    def get_test_data(self):\n",
    "        return self.testing_dataset.values, self.testing_labels.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# config\n",
    "no_of_iterations = 50000\n",
    "batch_size = 200\n",
    "\n",
    "## model config\n",
    "hidden_layer1_neurons = 60\n",
    "hidden_layer2_neurons = 30\n",
    "hidden_layer3_neurons = 20\n",
    "\n",
    "# DropOut\n",
    "pkeep_train = 0.75\n",
    "\n",
    "# number of features\n",
    "input_dim = len(dataset.columns) - 2\n",
    "\n",
    "# number of output classes\n",
    "output_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_provider = DataProvider(dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# custom decorator for Model\n",
    "#  - to make functions execute only the first time (every time the functions are called, the graph would be extended by new code)\n",
    "#  - name the variable scope for TF visualization\n",
    "def define_scope(function, scope=None):\n",
    "    attribute = '_cache_' + function.__name__\n",
    "\n",
    "    name = scope or function.__name__\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            with tf.variable_scope(name):\n",
    "                setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model description:\n",
    "Model consists of 3 hidden layers + 1 softmax output layer.\n",
    "Incorporates shootout while training to make inidividual neurons more independent on other layers and perform better.\n",
    "Incorporates learning rate decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, data, label, learning_rate):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.learning_rate = learning_rate\n",
    "        self.prediction\n",
    "        self.optimize\n",
    "        \n",
    "    @define_scope\n",
    "    def prediction(self):        \n",
    "        # weights + biases\n",
    "        w1 = tf.Variable(tf.truncated_normal([input_dim, hidden_layer1_neurons], stddev=0.0001))\n",
    "        b1 = tf.Variable(tf.ones([hidden_layer1_neurons]))\n",
    "\n",
    "        w2 = tf.Variable(tf.truncated_normal([hidden_layer1_neurons, hidden_layer2_neurons], stddev=0.0001))\n",
    "        b2 = tf.Variable(tf.ones([hidden_layer2_neurons]))\n",
    "\n",
    "        w3 = tf.Variable(tf.truncated_normal([hidden_layer2_neurons, hidden_layer3_neurons], stddev=0.0001))\n",
    "        b3 = tf.Variable(tf.ones([hidden_layer3_neurons]))\n",
    "        \n",
    "        w4 = tf.Variable(tf.truncated_normal([hidden_layer3_neurons, output_dim], stddev=0.0001))\n",
    "        b4 = tf.Variable(tf.ones([output_dim]))\n",
    "        \n",
    "        # hidden layers\n",
    "        Y1 = tf.nn.relu(tf.matmul(self.data, w1) + b1)\n",
    "        Y1d = tf.nn.dropout(Y1, pkeep)\n",
    "        Y2 = tf.nn.relu(tf.matmul(Y1, w2) + b2)\n",
    "        Y2d = tf.nn.dropout(Y2, pkeep)\n",
    "        Y3 = tf.nn.relu(tf.matmul(Y2, w3) + b3)\n",
    "        Y3d = tf.nn.dropout(Y3, pkeep)\n",
    "        \n",
    "        # softmax layer\n",
    "        return tf.nn.softmax(tf.matmul(Y3d, w4) + b4)\n",
    "    \n",
    "    @define_scope\n",
    "    def optimize(self):\n",
    "        # compute cost function and minimize\n",
    "        cross_entropy = -tf.reduce_sum(self.label * tf.log(self.prediction))\n",
    "        return tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy), cross_entropy\n",
    "    \n",
    "    @define_scope\n",
    "    def error(self):\n",
    "        mistakes = tf.equal(tf.argmax(self.label, 1), tf.argmax(self.prediction, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(mistakes, tf.float32))\n",
    "        loss = -tf.reduce_sum(self.label * tf.log(self.prediction))\n",
    "        return accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data feed\n",
    "X = tf.placeholder(tf.float32, [None, input_dim])\n",
    "_Y = tf.placeholder(tf.float32, [None, output_dim])\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "\n",
    "# DropOut: feed in 1 when testing, 0.75 when training\n",
    "pkeep = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(data=X, label=_Y, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "accuracy = []\n",
    "_loss = []\n",
    "for i in range(no_of_iterations):\n",
    "    # execute training step\n",
    "    # optimizer learning rate decay\n",
    "    lrmax = 0.001\n",
    "    lrmin = 0.00001\n",
    "    lr = lrmin + (lrmax - lrmin) * math.exp(-i / 2000)\n",
    "    \n",
    "    data_batch, label_batch = data_provider.next_batch_train()\n",
    "    sess.run(model.optimize, feed_dict={X: data_batch, _Y: label_batch, learning_rate:lr, pkeep: pkeep_train})\n",
    "    \n",
    "    if i % 500 == 0:\n",
    "        # compute accuracy\n",
    "        data_batch, label_batch = data_provider.get_test_data()\n",
    "        acc, loss = sess.run(model.error, feed_dict={X: data_batch, _Y: label_batch, pkeep: 1})\n",
    "        accuracy.append(acc)\n",
    "        _loss.append(loss)\n",
    "        print('---epoch {}---\\naccuracy: {}, loss: {}'.format(i // 500, acc, loss))\n",
    "        \n",
    "\n",
    "# accuracy on test data\n",
    "data_batch, label_batch = data_provider.get_test_data()\n",
    "acc, loss = sess.run(model.error, feed_dict={X: data_batch, _Y: label_batch, pkeep: 1})\n",
    "accuracy.append(acc)\n",
    "_loss.append(loss)\n",
    "print('Training finished\\naccuracy: {}, loss: {}'.format(acc, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,6))\n",
    "\n",
    "# accuracy\n",
    "plt.subplot(211)\n",
    "plt.plot(accuracy)\n",
    "\n",
    "# loss\n",
    "plt.subplot(212)\n",
    "plt.plot(np.log(_loss))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With current settings the deep neural network performs with accuracy close to 70%. This number could be potentially further increased by introducing more data (more features or larger training dataset) or by tweaking Model parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
